{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYAFCaNTLcJIVcJnukknFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ROYALKINGISK/ML-LAB/blob/main/ML_BL_EN_U4CSE21070_LAB8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "szRNyfPT1TAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4cf8da-ceb2-4a8f-cb02-df5f73d71e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Counts:\n",
            "yes    9\n",
            "no     5\n",
            "Name: buys_computer, dtype: int64\n",
            "\n",
            "Prior Probabilities:\n",
            "yes    0.642857\n",
            "no     0.357143\n",
            "Name: buys_computer, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#A1\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with your actual dataset\n",
        "data = {\n",
        "    'age': ['<=30','<=30','31...40','>40','>40','>40','31...40','<=30','<=30','>40','<=30','31...40','31...40','>40'],\n",
        "    'income': ['high', 'high','high','medium','low','low','low','medium','low','mediun','medium','medium','high','medium'],\n",
        "    'student': ['no', 'no', 'no', 'no','yes', 'yes', 'yes','no', 'yes','yes','yes', 'no', 'yes', 'no'],\n",
        "    'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'fair','fair','excellent', 'excellent','fair','excellent'],\n",
        "    'buys_computer': ['no', 'no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Count the occurrences of each class\n",
        "class_counts = df['buys_computer'].value_counts()\n",
        "\n",
        "# Calculate prior probability for each class\n",
        "prior_probabilities = class_counts / len(df)\n",
        "\n",
        "# Display the results\n",
        "print(\"Class Counts:\")\n",
        "print(class_counts)\n",
        "print(\"\\nPrior Probabilities:\")\n",
        "print(prior_probabilities)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A2\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import numpy as np\n",
        "age_mapping = {'<=30': 1, '31...40': 2, '>40': 3}\n",
        "df['age_numeric'] = df['age'].map(age_mapping)\n",
        "\n",
        "# Map income categories to numerical values\n",
        "income_mapping = {'low': 1, 'medium': 2, 'high': 3}\n",
        "df['income_numeric'] = df['income'].map(income_mapping)\n",
        "\n",
        "# Iterate through features and calculate class conditional densities\n",
        "for feature in ['age_numeric', 'income_numeric', 'student', 'credit_rating']:\n",
        "    for class_label in df['buys_computer'].unique():\n",
        "        # Extract data for the specific class\n",
        "        subset = df[df['buys_computer'] == class_label][feature].values.reshape(-1, 1)\n",
        "\n",
        "        # Use kernel density estimation\n",
        "        kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
        "        kde.fit(subset)\n",
        "\n",
        "        # Generate some sample data points\n",
        "        sample_points = np.linspace(df[feature].min(), df[feature].max(), 100).reshape(-1, 1)\n",
        "\n",
        "        # Calculate the log density for the sample points\n",
        "        log_density = kde.score_samples(sample_points)\n",
        "\n",
        "        # Display the results\n",
        "        print(f\"Class: {class_label}, Feature: {feature}\")\n",
        "        print(\"Sample Points:\", sample_points.flatten())\n",
        "        print(\"Log Density:\", log_density)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bNV2ObaN5eip",
        "outputId": "009f9b6b-7014-431b-d000-124b5cd8defe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: no, Feature: age_numeric\n",
            "Sample Points: [1.         1.02020202 1.04040404 1.06060606 1.08080808 1.1010101\n",
            " 1.12121212 1.14141414 1.16161616 1.18181818 1.2020202  1.22222222\n",
            " 1.24242424 1.26262626 1.28282828 1.3030303  1.32323232 1.34343434\n",
            " 1.36363636 1.38383838 1.4040404  1.42424242 1.44444444 1.46464646\n",
            " 1.48484848 1.50505051 1.52525253 1.54545455 1.56565657 1.58585859\n",
            " 1.60606061 1.62626263 1.64646465 1.66666667 1.68686869 1.70707071\n",
            " 1.72727273 1.74747475 1.76767677 1.78787879 1.80808081 1.82828283\n",
            " 1.84848485 1.86868687 1.88888889 1.90909091 1.92929293 1.94949495\n",
            " 1.96969697 1.98989899 2.01010101 2.03030303 2.05050505 2.07070707\n",
            " 2.09090909 2.11111111 2.13131313 2.15151515 2.17171717 2.19191919\n",
            " 2.21212121 2.23232323 2.25252525 2.27272727 2.29292929 2.31313131\n",
            " 2.33333333 2.35353535 2.37373737 2.39393939 2.41414141 2.43434343\n",
            " 2.45454545 2.47474747 2.49494949 2.51515152 2.53535354 2.55555556\n",
            " 2.57575758 2.5959596  2.61616162 2.63636364 2.65656566 2.67676768\n",
            " 2.6969697  2.71717172 2.73737374 2.75757576 2.77777778 2.7979798\n",
            " 2.81818182 2.83838384 2.85858586 2.87878788 2.8989899  2.91919192\n",
            " 2.93939394 2.95959596 2.97979798 3.        ]\n",
            "Log Density: [-1.34338142 -1.3401791  -1.33725675 -1.33461001 -1.33223444 -1.33012546\n",
            " -1.32827839 -1.32668846 -1.32535077 -1.3242603  -1.32341195 -1.3228005\n",
            " -1.32242062 -1.3222669  -1.32233381 -1.32261574 -1.32310697 -1.32380172\n",
            " -1.3246941  -1.32577816 -1.32704787 -1.32849714 -1.33011982 -1.33190969\n",
            " -1.33386051 -1.33596599 -1.33821981 -1.34061564 -1.34314714 -1.34580795\n",
            " -1.34859175 -1.35149223 -1.3545031  -1.35761815 -1.36083119 -1.36413613\n",
            " -1.36752695 -1.37099773 -1.37454265 -1.37815604 -1.38183234 -1.38556616\n",
            " -1.38935227 -1.3931856  -1.39706131 -1.40097474 -1.40492145 -1.40889723\n",
            " -1.41289813 -1.41692044 -1.42096071 -1.42501578 -1.42908278 -1.43315911\n",
            " -1.43724251 -1.44133098 -1.4454229  -1.44951691 -1.45361202 -1.45770756\n",
            " -1.46180316 -1.46589883 -1.46999489 -1.47409198 -1.47819109 -1.48229354\n",
            " -1.48640094 -1.49051525 -1.49463873 -1.49877394 -1.50292374 -1.50709127\n",
            " -1.51127995 -1.51549349 -1.51973581 -1.5240111  -1.5283238  -1.53267853\n",
            " -1.53708015 -1.54153368 -1.54604437 -1.55061758 -1.55525885 -1.55997387\n",
            " -1.56476844 -1.56964846 -1.57461995 -1.579689   -1.58486179 -1.59014454\n",
            " -1.59554352 -1.60106504 -1.60671543 -1.61250104 -1.61842821 -1.62450327\n",
            " -1.63073255 -1.63712234 -1.64367888 -1.6504084 ]\n",
            "\n",
            "Class: yes, Feature: age_numeric\n",
            "Sample Points: [1.         1.02020202 1.04040404 1.06060606 1.08080808 1.1010101\n",
            " 1.12121212 1.14141414 1.16161616 1.18181818 1.2020202  1.22222222\n",
            " 1.24242424 1.26262626 1.28282828 1.3030303  1.32323232 1.34343434\n",
            " 1.36363636 1.38383838 1.4040404  1.42424242 1.44444444 1.46464646\n",
            " 1.48484848 1.50505051 1.52525253 1.54545455 1.56565657 1.58585859\n",
            " 1.60606061 1.62626263 1.64646465 1.66666667 1.68686869 1.70707071\n",
            " 1.72727273 1.74747475 1.76767677 1.78787879 1.80808081 1.82828283\n",
            " 1.84848485 1.86868687 1.88888889 1.90909091 1.92929293 1.94949495\n",
            " 1.96969697 1.98989899 2.01010101 2.03030303 2.05050505 2.07070707\n",
            " 2.09090909 2.11111111 2.13131313 2.15151515 2.17171717 2.19191919\n",
            " 2.21212121 2.23232323 2.25252525 2.27272727 2.29292929 2.31313131\n",
            " 2.33333333 2.35353535 2.37373737 2.39393939 2.41414141 2.43434343\n",
            " 2.45454545 2.47474747 2.49494949 2.51515152 2.53535354 2.55555556\n",
            " 2.57575758 2.5959596  2.61616162 2.63636364 2.65656566 2.67676768\n",
            " 2.6969697  2.71717172 2.73737374 2.75757576 2.77777778 2.7979798\n",
            " 2.81818182 2.83838384 2.85858586 2.87878788 2.8989899  2.91919192\n",
            " 2.93939394 2.95959596 2.97979798 3.        ]\n",
            "Log Density: [-1.54087606 -1.5274627  -1.51429792 -1.50138099 -1.48871121 -1.47628789\n",
            " -1.46411035 -1.45217794 -1.44049002 -1.42904598 -1.4178452  -1.40688712\n",
            " -1.39617117 -1.3856968  -1.37546349 -1.36547073 -1.35571803 -1.34620492\n",
            " -1.33693095 -1.3278957  -1.31909875 -1.31053971 -1.3022182  -1.29413387\n",
            " -1.28628639 -1.27867544 -1.27130073 -1.26416198 -1.25725894 -1.25059137\n",
            " -1.24415905 -1.23796179 -1.23199941 -1.22627176 -1.22077868 -1.21552008\n",
            " -1.21049584 -1.20570589 -1.20115018 -1.19682865 -1.1927413  -1.18888813\n",
            " -1.18526915 -1.1818844  -1.17873395 -1.17581787 -1.17313627 -1.17068926\n",
            " -1.16847699 -1.1664996  -1.16475729 -1.16325024 -1.16197869 -1.16094285\n",
            " -1.160143   -1.1595794  -1.15925236 -1.15916219 -1.15930923 -1.15969382\n",
            " -1.16031635 -1.16117721 -1.16227681 -1.16361559 -1.16519399 -1.16701248\n",
            " -1.16907156 -1.17137174 -1.17391354 -1.1766975  -1.17972419 -1.1829942\n",
            " -1.18650813 -1.19026659 -1.19427023 -1.19851969 -1.20301567 -1.20775883\n",
            " -1.21274991 -1.21798961 -1.22347869 -1.22921791 -1.23520804 -1.24144987\n",
            " -1.24794423 -1.25469192 -1.26169381 -1.26895074 -1.27646358 -1.28423323\n",
            " -1.29226059 -1.30054658 -1.30909212 -1.31789817 -1.32696568 -1.33629563\n",
            " -1.345889   -1.35574679 -1.36587001 -1.37625969]\n",
            "\n",
            "Class: no, Feature: income_numeric\n",
            "Sample Points: [1.         1.02020202 1.04040404 1.06060606 1.08080808 1.1010101\n",
            " 1.12121212 1.14141414 1.16161616 1.18181818 1.2020202  1.22222222\n",
            " 1.24242424 1.26262626 1.28282828 1.3030303  1.32323232 1.34343434\n",
            " 1.36363636 1.38383838 1.4040404  1.42424242 1.44444444 1.46464646\n",
            " 1.48484848 1.50505051 1.52525253 1.54545455 1.56565657 1.58585859\n",
            " 1.60606061 1.62626263 1.64646465 1.66666667 1.68686869 1.70707071\n",
            " 1.72727273 1.74747475 1.76767677 1.78787879 1.80808081 1.82828283\n",
            " 1.84848485 1.86868687 1.88888889 1.90909091 1.92929293 1.94949495\n",
            " 1.96969697 1.98989899 2.01010101 2.03030303 2.05050505 2.07070707\n",
            " 2.09090909 2.11111111 2.13131313 2.15151515 2.17171717 2.19191919\n",
            " 2.21212121 2.23232323 2.25252525 2.27272727 2.29292929 2.31313131\n",
            " 2.33333333 2.35353535 2.37373737 2.39393939 2.41414141 2.43434343\n",
            " 2.45454545 2.47474747 2.49494949 2.51515152 2.53535354 2.55555556\n",
            " 2.57575758 2.5959596  2.61616162 2.63636364 2.65656566 2.67676768\n",
            " 2.6969697  2.71717172 2.73737374 2.75757576 2.77777778 2.7979798\n",
            " 2.81818182 2.83838384 2.85858586 2.87878788 2.8989899  2.91919192\n",
            " 2.93939394 2.95959596 2.97979798 3.        ]\n",
            "Log Density: [-1.61861422 -1.60446149 -1.59054241 -1.57685614 -1.56340187 -1.55017878\n",
            " -1.53718612 -1.52442313 -1.51188909 -1.4995833  -1.48750509 -1.47565381\n",
            " -1.46402883 -1.45262957 -1.44145545 -1.43050593 -1.4197805  -1.40927866\n",
            " -1.39899997 -1.38894399 -1.37911031 -1.36949855 -1.36010839 -1.35093948\n",
            " -1.34199156 -1.33326434 -1.32475761 -1.31647117 -1.30840483 -1.30055846\n",
            " -1.29293194 -1.28552518 -1.27833815 -1.2713708  -1.26462314 -1.25809521\n",
            " -1.25178708 -1.24569882 -1.23983058 -1.23418249 -1.22875475 -1.22354756\n",
            " -1.21856116 -1.21379582 -1.20925185 -1.20492956 -1.20082931 -1.19695149\n",
            " -1.19329651 -1.18986482 -1.18665686 -1.18367316 -1.18091422 -1.17838059\n",
            " -1.17607286 -1.17399163 -1.17213753 -1.17051121 -1.16911335 -1.16794467\n",
            " -1.16700589 -1.16629776 -1.16582108 -1.16557664 -1.16556527 -1.16578782\n",
            " -1.16624517 -1.1669382  -1.16786784 -1.16903502 -1.17044071 -1.17208588\n",
            " -1.17397153 -1.17609868 -1.17846837 -1.18108165 -1.18393959 -1.18704329\n",
            " -1.19039386 -1.19399242 -1.19784011 -1.20193808 -1.20628751 -1.21088958\n",
            " -1.21574549 -1.22085644 -1.22622367 -1.23184841 -1.2377319  -1.24387541\n",
            " -1.2502802  -1.25694755 -1.26387876 -1.27107511 -1.27853791 -1.28626847\n",
            " -1.29426811 -1.30253817 -1.31107997 -1.31989484]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d64ed18ffbc0>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Use kernel density estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Generate some sample data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_kde.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandwidth_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKernelDensity does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A3\n",
        "from scipy.stats import chi2_contingency, pearsonr\n",
        "categorical_features = ['student', 'credit_rating', 'buys_computer']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    contingency_table = pd.crosstab(df[feature], df['buys_computer'])\n",
        "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
        "    print(f\"Chi-squared test for independence between {feature} and buys_computer:\")\n",
        "    print(f\"Chi-squared statistic: {chi2}\")\n",
        "    print(f\"P-value: {p}\")\n",
        "    print(f\"{'*' * 40}\")\n",
        "\n",
        "# Numeric features: Pearson correlation coefficient\n",
        "numeric_features = ['age_numeric', 'income_numeric']\n",
        "\n",
        "for feature in numeric_features:\n",
        "    # Drop rows with missing values\n",
        "    valid_data = df.dropna(subset=[feature, 'age_numeric'])\n",
        "\n",
        "    correlation, p_value = pearsonr(valid_data[feature], valid_data['age_numeric'])\n",
        "    print(f\"Pearson correlation coefficient between {feature} and age_numeric:\")\n",
        "    print(f\"Correlation coefficient: {correlation}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "    print(f\"{'*' * 40}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFMfYh06avz",
        "outputId": "a09a276d-21bc-444c-a753-d0a0e8ea7186"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-squared test for independence between student and buys_computer:\n",
            "Chi-squared statistic: 1.2444444444444445\n",
            "P-value: 0.2646162170835855\n",
            "****************************************\n",
            "Chi-squared test for independence between credit_rating and buys_computer:\n",
            "Chi-squared statistic: 0.16203703703703706\n",
            "P-value: 0.6872879493480019\n",
            "****************************************\n",
            "Chi-squared test for independence between buys_computer and buys_computer:\n",
            "Chi-squared statistic: 9.983209876543212\n",
            "P-value: 0.0015797405840629584\n",
            "****************************************\n",
            "Pearson correlation coefficient between age_numeric and age_numeric:\n",
            "Correlation coefficient: 0.9999999999999999\n",
            "P-value: 0.0\n",
            "****************************************\n",
            "Pearson correlation coefficient between income_numeric and age_numeric:\n",
            "Correlation coefficient: -0.35507405735549946\n",
            "P-value: 0.23383709750448237\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A4\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df[['age', 'income', 'student', 'credit_rating']]\n",
        "y = df['buys_computer']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical features to a dictionary format\n",
        "train_features = X_train.to_dict(orient='records')\n",
        "test_features = X_test.to_dict(orient='records')\n",
        "\n",
        "# Vectorize features using DictVectorizer\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "X_train_vec = vectorizer.fit_transform(train_features)\n",
        "X_test_vec = vectorizer.transform(test_features)\n",
        "\n",
        "# Build a Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Naïve Bayes Classifier Results:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au7t3On16_Uy",
        "outputId": "f4b6bb00-29a3-4cf2-99c7-459c1e03b0c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naïve Bayes Classifier Results:\n",
            "Accuracy: 0.33\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          no       0.00      0.00      0.00         1\n",
            "         yes       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.25      0.25      0.25         3\n",
            "weighted avg       0.33      0.33      0.33         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A5\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Replace this with your actual dataset\n",
        "data = pd.read_csv(\"extracted_features_charrec.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(['class_name'], axis=1)\n",
        "y = df['class_name']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical features to a dictionary format\n",
        "train_features = X_train.to_dict(orient='records')\n",
        "test_features = X_test.to_dict(orient='records')\n",
        "\n",
        "# Vectorize features using DictVectorizer\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "X_train_vec = vectorizer.fit_transform(train_features)\n",
        "X_test_vec = vectorizer.transform(test_features)\n",
        "\n",
        "# Build a Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Naïve Bayes Classifier Results:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioejxm3F7Qjk",
        "outputId": "692f6f69-bb12-40d9-aaec-6a9534696bb3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naïve Bayes Classifier Results:\n",
            "Accuracy: 0.65\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        3333       0.45      0.94      0.61        36\n",
            "        3334       0.48      0.94      0.64        33\n",
            "        3335       0.33      0.72      0.45        39\n",
            "        3337       0.58      0.53      0.55        34\n",
            "        3342       0.52      0.57      0.54        28\n",
            "        3343       0.00      0.00      0.00        26\n",
            "        3346       0.48      0.57      0.52        21\n",
            "        3349       0.73      0.64      0.68        47\n",
            "        3350       0.00      0.00      0.00        20\n",
            "        3351       0.95      0.84      0.89        43\n",
            "        3352       0.00      0.00      0.00        22\n",
            "        3353       0.50      0.86      0.63        29\n",
            "        3354       0.88      0.77      0.82        30\n",
            "        3355       0.00      0.00      0.00        20\n",
            "        3356       0.50      0.84      0.63        32\n",
            "        3357       1.00      0.04      0.07        27\n",
            "        3358       0.00      0.00      0.00        23\n",
            "        3359       0.98      0.92      0.95        49\n",
            "        3360       0.68      0.95      0.79        60\n",
            "        3361       0.98      0.92      0.95        63\n",
            "        3362       0.86      0.91      0.89        56\n",
            "        3363       0.81      0.53      0.64        47\n",
            "        3364       0.53      0.30      0.38        33\n",
            "        3365       0.70      0.67      0.68        21\n",
            "        3366       0.77      0.87      0.82        23\n",
            "        3367       0.77      0.48      0.59        21\n",
            "        3368       0.61      0.41      0.49        34\n",
            "        3370       0.72      0.50      0.59        26\n",
            "        3371       0.62      0.95      0.75        19\n",
            "        3372       0.40      0.89      0.55        19\n",
            "        3373       0.79      0.46      0.58        24\n",
            "        3374       0.55      0.46      0.50        24\n",
            "        3375       0.63      0.52      0.57        23\n",
            "        3376       0.50      0.44      0.47        25\n",
            "        3377       0.72      0.75      0.73        24\n",
            "        3378       0.77      0.57      0.65        30\n",
            "        3379       0.76      0.83      0.79        23\n",
            "        3380       1.00      0.94      0.97        17\n",
            "        3381       1.00      0.67      0.80        33\n",
            "        3382       0.53      0.62      0.57        13\n",
            "        3383       0.94      0.71      0.81        21\n",
            "        3384       0.47      0.75      0.58        24\n",
            "        3385       0.50      0.71      0.59        17\n",
            "        3450       0.77      1.00      0.87        10\n",
            "        3451       0.64      0.78      0.70         9\n",
            "        3452       0.40      0.50      0.44         8\n",
            "        3453       0.82      0.75      0.78        12\n",
            "        3454       0.73      0.79      0.76        14\n",
            "\n",
            "    accuracy                           0.65      1332\n",
            "   macro avg       0.61      0.62      0.59      1332\n",
            "weighted avg       0.65      0.65      0.62      1332\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}